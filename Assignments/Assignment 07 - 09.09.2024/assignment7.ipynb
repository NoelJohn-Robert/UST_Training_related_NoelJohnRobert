{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0228604b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcfcc564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff5000e73c8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa8fc26",
   "metadata": {},
   "source": [
    "### a) Create a new Spark Session with new SparkConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32b06b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73faf344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "# setMaster sets spark ContextManager which is loca[cpu cores]\n",
    "config = SparkConf().setMaster('local[4]').setAppName(\"PySparkSession\")\n",
    "sc = SparkContext(conf=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2d7c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=PySparkSession>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a30888",
   "metadata": {},
   "source": [
    "### b) Create new instance of Spark SQL session and define new DataFrame using sales_data_sample.csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac6811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SQLSession\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "767ffb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff500065f28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aa7ebc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+-----+----------+-------+---------+---------------+----------------+--------+\n",
      "|ORDERNUMBER|QUANTITYORDERED|PRICEEACH|ORDERLINENUMBER|  SALES|      ORDERDATE| STATUS|QTR_ID|MONTH_ID|YEAR_ID|PRODUCTLINE|MSRP|PRODUCTCODE|        CUSTOMERNAME|           PHONE|        ADDRESSLINE1|ADDRESSLINE2|         CITY|STATE|POSTALCODE|COUNTRY|TERRITORY|CONTACTLASTNAME|CONTACTFIRSTNAME|DEALSIZE|\n",
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+-----+----------+-------+---------+---------------+----------------+--------+\n",
      "|      10107|             30|     95.7|              2| 2871.0| 2/24/2003 0:00|Shipped|     1|       2|   2003|Motorcycles|  95|   S10_1678|   Land of Toys Inc.|      2125557818|897 Long Airport ...|        null|          NYC|   NY|     10022|    USA|       NA|             Yu|            Kwai|   Small|\n",
      "|      10121|             34|    81.35|              5| 2765.9|  5/7/2003 0:00|Shipped|     2|       5|   2003|Motorcycles|  95|   S10_1678|  Reims Collectables|      26.47.1555|  59 rue de l'Abbaye|        null|        Reims| null|     51100| France|     EMEA|        Henriot|            Paul|   Small|\n",
      "|      10134|             41|    94.74|              2|3884.34|  7/1/2003 0:00|Shipped|     3|       7|   2003|Motorcycles|  95|   S10_1678|     Lyon Souveniers|+33 1 46 62 7555|27 rue du Colonel...|        null|        Paris| null|     75508| France|     EMEA|       Da Cunha|          Daniel|  Medium|\n",
      "|      10145|             45|    83.26|              6| 3746.7| 8/25/2003 0:00|Shipped|     3|       8|   2003|Motorcycles|  95|   S10_1678|   Toys4GrownUps.com|      6265557265|  78934 Hillside Dr.|        null|     Pasadena|   CA|     90003|    USA|       NA|          Young|           Julie|  Medium|\n",
      "|      10159|             49|    100.0|             14|5205.27|10/10/2003 0:00|Shipped|     4|      10|   2003|Motorcycles|  95|   S10_1678|Corporate Gift Id...|      6505551386|     7734 Strong St.|        null|San Francisco|   CA|      null|    USA|       NA|          Brown|           Julie|  Medium|\n",
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+-----+----------+-------+---------+---------------+----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df = spark.read.csv('file:///home/hadoop/Downloads/sales_data_sample.csv', header=True, inferSchema=True)\n",
    "sales_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2cb81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fb03e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ORDERNUMBER: integer (nullable = true)\n",
      " |-- QUANTITYORDERED: integer (nullable = true)\n",
      " |-- PRICEEACH: double (nullable = true)\n",
      " |-- ORDERLINENUMBER: integer (nullable = true)\n",
      " |-- SALES: double (nullable = true)\n",
      " |-- ORDERDATE: string (nullable = true)\n",
      " |-- STATUS: string (nullable = true)\n",
      " |-- QTR_ID: integer (nullable = true)\n",
      " |-- MONTH_ID: integer (nullable = true)\n",
      " |-- YEAR_ID: integer (nullable = true)\n",
      " |-- PRODUCTLINE: string (nullable = true)\n",
      " |-- MSRP: integer (nullable = true)\n",
      " |-- PRODUCTCODE: string (nullable = true)\n",
      " |-- CUSTOMERNAME: string (nullable = true)\n",
      " |-- PHONE: string (nullable = true)\n",
      " |-- ADDRESSLINE1: string (nullable = true)\n",
      " |-- ADDRESSLINE2: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- POSTALCODE: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      " |-- TERRITORY: string (nullable = true)\n",
      " |-- CONTACTLASTNAME: string (nullable = true)\n",
      " |-- CONTACTFIRSTNAME: string (nullable = true)\n",
      " |-- DEALSIZE: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71661390",
   "metadata": {},
   "source": [
    "### c) Find the shape of DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4330311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe is: (2823, 25)\n"
     ]
    }
   ],
   "source": [
    "rows = sales_df.count()\n",
    "columns = len(sales_df.columns)\n",
    "print(f'Shape of dataframe is: ({rows}, {columns})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de5e5a1",
   "metadata": {},
   "source": [
    "### d) Find the Summary of DataFrame for all numerical data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccc99e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|       ORDERNUMBER|  QUANTITYORDERED|         PRICEEACH|  ORDERLINENUMBER|             SALES|            QTR_ID|          MONTH_ID|           YEAR_ID|              MSRP|\n",
      "+-------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|              2823|             2823|              2823|             2823|              2823|              2823|              2823|              2823|              2823|\n",
      "|   mean|10258.725115125753|35.09280906836698| 83.65854410201929|6.466170740347148|  3553.88907190932|2.7176762309599716|7.0924548352816155|2003.8150903294368|100.71555083244775|\n",
      "| stddev|  92.0854775957196| 9.74144273706958|20.174276527840536| 4.22584096469094|1841.8651057401842| 1.203878088001756| 3.656633307661765|0.6996701541300869| 40.18791167720266|\n",
      "|    min|             10100|                6|             26.88|                1|            482.13|                 1|                 1|              2003|                33|\n",
      "|    25%|             10180|               27|              68.8|                3|           2203.11|                 2|                 4|              2003|                68|\n",
      "|    50%|             10262|               35|              95.7|                6|            3184.8|                 3|                 8|              2004|                99|\n",
      "|    75%|             10334|               43|             100.0|                9|            4508.0|                 4|                11|              2004|               124|\n",
      "|    max|             10425|               97|             100.0|               18|           14082.8|                 4|                12|              2005|               214|\n",
      "+-------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "numerical_columns = [field.name for field in sales_df.schema.fields if not isinstance(field.dataType, StringType)]\n",
    "sales_df.select(numerical_columns).summary().show()\n",
    "\n",
    "# INSIGHT: summary of the DataFrame for numerical columns shows count, min vlaue, max value, and quartile values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92739b4",
   "metadata": {},
   "source": [
    "### e) Identify and handle missing or null values in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45cde3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------+---------------+-----+---------+------+------+--------+-------+-----------+----+-----------+------------+-----+------------+------------+----+-----+----------+-------+---------+---------------+----------------+--------+\n",
      "|ORDERNUMBER|QUANTITYORDERED|PRICEEACH|ORDERLINENUMBER|SALES|ORDERDATE|STATUS|QTR_ID|MONTH_ID|YEAR_ID|PRODUCTLINE|MSRP|PRODUCTCODE|CUSTOMERNAME|PHONE|ADDRESSLINE1|ADDRESSLINE2|CITY|STATE|POSTALCODE|COUNTRY|TERRITORY|CONTACTLASTNAME|CONTACTFIRSTNAME|DEALSIZE|\n",
      "+-----------+---------------+---------+---------------+-----+---------+------+------+--------+-------+-----------+----+-----------+------------+-----+------------+------------+----+-----+----------+-------+---------+---------------+----------------+--------+\n",
      "|          0|              0|        0|              0|    0|        0|     0|     0|       0|      0|          0|   0|          0|           0|    0|           0|        2521|   0| 1486|        76|      0|        0|              0|               0|       0|\n",
      "+-----------+---------------+---------+---------------+-----+---------+------+------+--------+-------+-----------+----+-----------+------------+-----+------------+------------+----+-----+----------+-------+---------+---------------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.select([count(when(isnull(col), col)).alias(col) for col in sales_df.columns]).show()\n",
    "# ADDRESSLINE2, STATE, POSTALCODE - all have NULL values in them - this might need to be handled [dont drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65d42700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling null values with empty string\n",
    "sales_df1 = sales_df.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef233a94",
   "metadata": {},
   "source": [
    "### f) Calculate the total revenue generated per country by combining the columns QUANTITYORDERED and PRICEEACH using Spark DataFrame operations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88fdeeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|    COUNTRY|TOTALREVENUE|\n",
      "+-----------+------------+\n",
      "|     Sweden|    174264.1|\n",
      "|Philippines|    80291.17|\n",
      "|  Singapore|    227985.5|\n",
      "|    Germany|   178689.08|\n",
      "|     France|   919257.85|\n",
      "|    Belgium|    94528.88|\n",
      "|    Finland|    268714.7|\n",
      "|      Italy|   309402.87|\n",
      "|     Norway|    246115.8|\n",
      "|      Spain|  1021705.97|\n",
      "|    Denmark|   192747.63|\n",
      "|    Ireland|    43237.24|\n",
      "|        USA|  2986425.21|\n",
      "|         UK|   413203.34|\n",
      "|Switzerland|    93344.91|\n",
      "|     Canada|   193504.34|\n",
      "|      Japan|   153076.69|\n",
      "|  Australia|   521598.46|\n",
      "|    Austria|   172793.05|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queryResult = sales_df1.select(\n",
    "                col('COUNTRY'), (col('QUANTITYORDERED')*col('PRICEEACH')).alias('REVENUESUM')\n",
    "              ).groupBy(['COUNTRY']).agg(round(sum('REVENUESUM'), 3).alias('TOTALREVENUE'))\n",
    "queryResult.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b455ec",
   "metadata": {},
   "source": [
    "### g) Determine the top 5 products with the highest total sales revenue using Spark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49fbe568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "581d432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+\n",
      "|     PRODUCTLINE|TOTALREVENUE|\n",
      "+----------------+------------+\n",
      "|    Classic Cars|  3919615.66|\n",
      "|    Vintage Cars|  1903150.84|\n",
      "|     Motorcycles|  1166388.34|\n",
      "|Trucks and Buses|  1127789.84|\n",
      "|          Planes|   975003.57|\n",
      "+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queryResult = sales_df1.select(\n",
    "                col('PRODUCTLINE'), (col('SALES')).alias('TOTALREVENUE')\n",
    "              ).groupBy(['PRODUCTLINE']).agg(round(sum('TOTALREVENUE'), 3).alias('TOTALREVENUE'))\\\n",
    "              .orderBy(['TOTALREVENUE'], ascending=False)\n",
    "queryResult.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38d252",
   "metadata": {},
   "source": [
    "### h) Find the average order quantity for each product using groupBy and agg operations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ef5b588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|     PRODUCTLINE|AVG_QUANTITYORDERED|\n",
      "+----------------+-------------------+\n",
      "|     Motorcycles|             35.236|\n",
      "|    Vintage Cars|              34.71|\n",
      "|           Ships|             34.731|\n",
      "|Trucks and Buses|             35.804|\n",
      "|    Classic Cars|             35.152|\n",
      "|          Trains|             35.221|\n",
      "|          Planes|             35.056|\n",
      "+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queryResult = sales_df1.select(\n",
    "                col('PRODUCTLINE'), col('QUANTITYORDERED')\n",
    "              ).groupBy(['PRODUCTLINE']).\\\n",
    "              agg(round(avg('QUANTITYORDERED'), 3).alias('AVG_QUANTITYORDERED'))\n",
    "\n",
    "queryResult.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d26d59",
   "metadata": {},
   "source": [
    "### i) Using Spark DataFrame, filter orders where the SALES value exceeds $10,000 and sort the results by the ORDERDATE column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83086e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------------+\n",
      "|ORDERNUMBER|  SALES|          ORDERDATE|\n",
      "+-----------+-------+-------------------+\n",
      "|      10127|11279.2|2003-06-03 00:00:00|\n",
      "|      10150|10993.5|2003-09-19 00:00:00|\n",
      "|      10247|10606.2|2004-05-05 00:00:00|\n",
      "|      10304|10172.7|2004-10-11 00:00:00|\n",
      "|      10312|11623.7|2004-10-21 00:00:00|\n",
      "|      10322|12536.5|2004-11-04 00:00:00|\n",
      "|      10333|11336.7|2004-11-18 00:00:00|\n",
      "|      10339|10758.0|2004-11-23 00:00:00|\n",
      "|      10375|10039.6|2005-02-03 00:00:00|\n",
      "|      10388|10066.6|2005-03-03 00:00:00|\n",
      "|      10403|11886.6|2005-04-08 00:00:00|\n",
      "|      10405|11739.7|2005-04-14 00:00:00|\n",
      "|      10406|10468.9|2005-04-15 00:00:00|\n",
      "|      10407|14082.8|2005-04-22 00:00:00|\n",
      "|      10412|11887.8|2005-05-03 00:00:00|\n",
      "|      10424|12001.0|2005-05-31 00:00:00|\n",
      "+-----------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sales_df1.filter(col('SALES')>10000).show()\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "sales_df1.withColumn('ORDERDATE', to_timestamp(col('ORDERDATE'), 'MM/dd/yyyy H:mm'))\\\n",
    "            .select(col('ORDERNUMBER'), col('SALES'), col('ORDERDATE'))\\\n",
    "            .filter(col('SALES')>10000)\\\n",
    "            .orderBy('ORDERDATE').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396befa",
   "metadata": {},
   "source": [
    "### j) Filter out rows where the STATUS is 'Cancelled' and calculate the total sales from the remaining orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "196969c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|TOTAL SALES|\n",
      "+-----------+\n",
      "| 9838141.37|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df1.filter(col('STATUS') != 'Cancelled').agg(round(sum('SALES'), 3).alias('TOTAL SALES')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08842198",
   "metadata": {},
   "source": [
    "### k) Use Spark Data Frame transformations to derive the yearly sales for each customer (CUSTOMERNAME) based on the ORDERDATE column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15337240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+\n",
      "|        CUSTOMERNAME|YEAR|AVG_QUANTITYORDERED|\n",
      "+--------------------+----+-------------------+\n",
      "|      AV Stores, Co.|2003|           51017.92|\n",
      "|      AV Stores, Co.|2004|          106789.89|\n",
      "|        Alpha Cognac|2003|           55349.32|\n",
      "|        Alpha Cognac|2005|           15139.12|\n",
      "|  Amica Models & Co.|2004|           94117.26|\n",
      "|Anna's Decoration...|2003|           88983.71|\n",
      "|Anna's Decoration...|2005|           65012.42|\n",
      "|   Atelier graphique|2003|            16560.3|\n",
      "|   Atelier graphique|2004|            7619.66|\n",
      "|Australian Collec...|2003|           37878.55|\n",
      "|Australian Collec...|2004|           12334.82|\n",
      "|Australian Collec...|2005|           14378.09|\n",
      "|Australian Collec...|2003|           60135.84|\n",
      "|Australian Collec...|2004|          140859.57|\n",
      "|Australian Gift N...|2003|           37739.09|\n",
      "|Australian Gift N...|2005|           21730.03|\n",
      "|  Auto Assoc. & Cie.|2004|           64834.32|\n",
      "|    Auto Canal Petit|2004|           79103.86|\n",
      "|    Auto Canal Petit|2005|            14066.8|\n",
      "|Auto-Moto Classic...|2003|            7277.35|\n",
      "+--------------------+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queryResult = sales_df1.withColumn('ORDERDATE', to_timestamp(col('ORDERDATE'), 'MM/dd/yyyy H:mm'))\\\n",
    "              .select(col('CUSTOMERNAME'), year('ORDERDATE').alias('YEAR'), col('SALES'))\\\n",
    "              .groupBy(['CUSTOMERNAME', 'YEAR'])\\\n",
    "              .agg(round(sum('SALES'), 3).alias('AVG_QUANTITYORDERED'))\\\n",
    "              .orderBy(['CUSTOMERNAME', 'YEAR'])\n",
    "\n",
    "queryResult.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ffada",
   "metadata": {},
   "source": [
    "### l) Add a new column to the DataFrame that categorizes orders as 'High', 'Medium', or 'Low' sales based on the SALES value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3228fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------+\n",
      "|ORDERNUMBER| SALES|SALES_CATEGORY|\n",
      "+-----------+------+--------------+\n",
      "|      10425|482.13|           Low|\n",
      "|      10407|541.14|           Low|\n",
      "|      10408|553.95|           Low|\n",
      "|      10280| 577.6|           Low|\n",
      "|      10419|640.05|           Low|\n",
      "|      10264| 651.8|           Low|\n",
      "|      10420|652.35|           Low|\n",
      "|      10214| 683.8|           Low|\n",
      "|      10304| 694.6|           Low|\n",
      "|      10344| 703.6|           Low|\n",
      "|      10110| 710.2|           Low|\n",
      "|      10135| 717.4|           Low|\n",
      "|      10114|721.44|           Low|\n",
      "|      10358| 728.4|           Low|\n",
      "|      10375|733.11|           Low|\n",
      "|      10193|759.46|           Low|\n",
      "|      10203| 777.0|           Low|\n",
      "|      10409|785.64|           Low|\n",
      "|      10281| 813.2|           Low|\n",
      "|      10156| 820.4|           Low|\n",
      "+-----------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import ntile\n",
    "\n",
    "windowSpec = Window.orderBy('SALES')\n",
    "sales_df1.withColumn('quantile',ntile(3).over(windowSpec))\\\n",
    "         .select(col('ORDERNUMBER'), col('SALES'), col('quantile'))\\\n",
    "         .withColumn(\n",
    "             'SALES_CATEGORY',\n",
    "             when(col('quantile')==1, 'Low')\\\n",
    "             .when(col('quantile')==2, 'Medium')\\\n",
    "             .when(col('quantile')==3, 'High')\n",
    "         )\\\n",
    "         .select(col('ORDERNUMBER'), col('SALES'), col('SALES_CATEGORY'))\\\n",
    "         .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d677e73",
   "metadata": {},
   "source": [
    "### m) Assume , If you have another DataFrame with customer demographic data, how would you perform a join to compute the total sales per demographic group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "253f67f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+--------------------+---------+-------------+\n",
      "|ORDERNUMBER|QUANTITYORDERED|PRICEEACH|ORDERLINENUMBER|  SALES|      ORDERDATE| STATUS|QTR_ID|MONTH_ID|YEAR_ID|PRODUCTLINE|MSRP|PRODUCTCODE|        CUSTOMERNAME|           PHONE|        ADDRESSLINE1|ADDRESSLINE2|         CITY|   STATE|POSTALCODE|  COUNTRY|TERRITORY|CONTACTLASTNAME|CONTACTFIRSTNAME|DEALSIZE|        CUSTOMERNAME|ESTD_YEAR|NUM_EMPLOYEES|\n",
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+--------------------+---------+-------------+\n",
      "|      10107|             30|     95.7|              2| 2871.0| 2/24/2003 0:00|Shipped|     1|       2|   2003|Motorcycles|  95|   S10_1678|   Land of Toys Inc.|      2125557818|897 Long Airport ...|            |          NYC|      NY|     10022|      USA|       NA|             Yu|            Kwai|   Small|   Land of Toys Inc.|     2009|           75|\n",
      "|      10121|             34|    81.35|              5| 2765.9|  5/7/2003 0:00|Shipped|     2|       5|   2003|Motorcycles|  95|   S10_1678|  Reims Collectables|      26.47.1555|  59 rue de l'Abbaye|            |        Reims|        |     51100|   France|     EMEA|        Henriot|            Paul|   Small|  Reims Collectables|     2015|           70|\n",
      "|      10134|             41|    94.74|              2|3884.34|  7/1/2003 0:00|Shipped|     3|       7|   2003|Motorcycles|  95|   S10_1678|     Lyon Souveniers|+33 1 46 62 7555|27 rue du Colonel...|            |        Paris|        |     75508|   France|     EMEA|       Da Cunha|          Daniel|  Medium|     Lyon Souveniers|     1995|           60|\n",
      "|      10145|             45|    83.26|              6| 3746.7| 8/25/2003 0:00|Shipped|     3|       8|   2003|Motorcycles|  95|   S10_1678|   Toys4GrownUps.com|      6265557265|  78934 Hillside Dr.|            |     Pasadena|      CA|     90003|      USA|       NA|          Young|           Julie|  Medium|   Toys4GrownUps.com|     2008|          120|\n",
      "|      10159|             49|    100.0|             14|5205.27|10/10/2003 0:00|Shipped|     4|      10|   2003|Motorcycles|  95|   S10_1678|Corporate Gift Id...|      6505551386|     7734 Strong St.|            |San Francisco|      CA|          |      USA|       NA|          Brown|           Julie|  Medium|Corporate Gift Id...|     1999|           50|\n",
      "|      10168|             36|    96.66|              1|3479.76|10/28/2003 0:00|Shipped|     4|      10|   2003|Motorcycles|  95|   S10_1678|Technics Stores Inc.|      6505556809|   9408 Furth Circle|            |   Burlingame|      CA|     94217|      USA|       NA|         Hirano|            Juri|  Medium|Technics Stores Inc.|     2010|           80|\n",
      "|      10180|             29|    86.13|              9|2497.77|11/11/2003 0:00|Shipped|     4|      11|   2003|Motorcycles|  95|   S10_1678|Daedalus Designs ...|      20.16.1555|184, chausse de T...|            |        Lille|        |     59000|   France|     EMEA|          Rance|         Martine|   Small|Daedalus Designs ...|     2002|          100|\n",
      "|      10188|             48|    100.0|              1|5512.32|11/18/2003 0:00|Shipped|     4|      11|   2003|Motorcycles|  95|   S10_1678|        Herkku Gifts|   +47 2267 3215|Drammen 121, PR 7...|            |       Bergen|        |    N 5804|   Norway|     EMEA|         Oeztan|          Veysel|  Medium|        Herkku Gifts|     2000|           90|\n",
      "|      10201|             22|    98.57|              2|2168.54| 12/1/2003 0:00|Shipped|     4|      12|   2003|Motorcycles|  95|   S10_1678|     Mini Wheels Co.|      6505555787|5557 North Pendal...|            |San Francisco|      CA|          |      USA|       NA|         Murphy|           Julie|   Small|     Mini Wheels Co.|     2009|           35|\n",
      "|      10211|             41|    100.0|             14|4708.44| 1/15/2004 0:00|Shipped|     1|       1|   2004|Motorcycles|  95|   S10_1678|    Auto Canal Petit|  (1) 47.55.6555|   25, rue Lauriston|            |        Paris|        |     75016|   France|     EMEA|        Perrier|       Dominique|  Medium|    Auto Canal Petit|     2002|           55|\n",
      "|      10223|             37|    100.0|              1|3965.66| 2/20/2004 0:00|Shipped|     1|       2|   2004|Motorcycles|  95|   S10_1678|Australian Collec...|    03 9520 4555|   636 St Kilda Road|     Level 3|    Melbourne|Victoria|      3004|Australia|     APAC|       Ferguson|           Peter|  Medium|Australian Collec...|     2007|           60|\n",
      "|      10237|             23|    100.0|              7|2333.12|  4/5/2004 0:00|Shipped|     2|       4|   2004|Motorcycles|  95|   S10_1678|     Vitachrome Inc.|      2125551500|   2678 Kingston Rd.|   Suite 101|          NYC|      NY|     10022|      USA|       NA|          Frick|         Michael|   Small|     Vitachrome Inc.|     2015|           25|\n",
      "|      10251|             28|    100.0|              2|3188.64| 5/18/2004 0:00|Shipped|     2|       5|   2004|Motorcycles|  95|   S10_1678|Tekni Collectable...|      2015559350|       7476 Moss Rd.|            |       Newark|      NJ|     94019|      USA|       NA|          Brown|         William|  Medium|Tekni Collectable...|     2007|           85|\n",
      "|      10263|             34|    100.0|              2|3676.76| 6/28/2004 0:00|Shipped|     2|       6|   2004|Motorcycles|  95|   S10_1678|     Gift Depot Inc.|      2035552570| 25593 South Bay Ln.|            |  Bridgewater|      CT|     97562|      USA|       NA|           King|           Julie|  Medium|     Gift Depot Inc.|     2002|           80|\n",
      "|      10275|             45|    92.83|              1|4177.35| 7/23/2004 0:00|Shipped|     3|       7|   2004|Motorcycles|  95|   S10_1678|   La Rochelle Gifts|      40.67.8555|67, rue des Cinqu...|            |       Nantes|        |     44000|   France|     EMEA|        Labrune|          Janine|  Medium|   La Rochelle Gifts|     1999|           55|\n",
      "|      10285|             36|    100.0|              6|4099.68| 8/27/2004 0:00|Shipped|     3|       8|   2004|Motorcycles|  95|   S10_1678|Marta's Replicas Co.|      6175558555| 39323 Spinnaker Dr.|            |    Cambridge|      MA|     51247|      USA|       NA|      Hernandez|           Marta|  Medium|Marta's Replicas Co.|     2009|           45|\n",
      "|      10299|             23|    100.0|              9|2597.39| 9/30/2004 0:00|Shipped|     3|       9|   2004|Motorcycles|  95|   S10_1678|Toys of Finland, Co.|     90-224 8555|       Keskuskatu 45|            |     Helsinki|        |     21240|  Finland|     EMEA|      Karttunen|           Matti|   Small|Toys of Finland, Co.|     2014|           50|\n",
      "|      10309|             41|    100.0|              5|4394.38|10/15/2004 0:00|Shipped|     4|      10|   2004|Motorcycles|  95|   S10_1678|  Baane Mini Imports|      07-98 9555|Erling Skakkes ga...|            |      Stavern|        |      4110|   Norway|     EMEA|     Bergulfsen|           Jonas|  Medium|  Baane Mini Imports|     1997|           60|\n",
      "|      10318|             46|    94.74|              1|4358.04| 11/2/2004 0:00|Shipped|     4|      11|   2004|Motorcycles|  95|   S10_1678|Diecast Classics ...|      2155551555|    7586 Pompton St.|            |    Allentown|      PA|     70267|      USA|       NA|             Yu|           Kyung|  Medium|Diecast Classics ...|     2006|           65|\n",
      "|      10329|             42|    100.0|              1|4396.14|11/15/2004 0:00|Shipped|     4|      11|   2004|Motorcycles|  95|   S10_1678|   Land of Toys Inc.|      2125557818|897 Long Airport ...|            |          NYC|      NY|     10022|      USA|       NA|             Yu|            Kwai|  Medium|   Land of Toys Inc.|     2009|           75|\n",
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+--------------------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_demographic_data = spark.read.csv('file:///home/hadoop/Downloads/sales_data_customer_demographic.csv', header=True, inferSchema=True)\n",
    "joined_df = sales_df1.join(customer_demographic_data, sales_df1.CUSTOMERNAME==customer_demographic_data.CUSTOMERNAME, \"inner\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f0d033",
   "metadata": {},
   "source": [
    "### n) Can you implement a cumulative distribution function (CDF) over the SALES value for each CUSTOMERNAME? What insights can you gather from analyzing the CDF distribution for each customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efec9259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------+---------+\n",
      "|ORDERNUMBER|       CUSTOMERNAME|  SALES|cume_dist|\n",
      "+-----------+-------------------+-------+---------+\n",
      "|      10141|Suominen Souveniers| 891.03|  0.03333|\n",
      "|      10141|Suominen Souveniers| 1086.6|  0.06667|\n",
      "|      10141|Suominen Souveniers|1103.76|      0.1|\n",
      "|      10363|Suominen Souveniers|1629.04|  0.13333|\n",
      "|      10247|Suominen Souveniers| 1988.4|  0.16667|\n",
      "|      10141|Suominen Souveniers|2140.11|      0.2|\n",
      "|      10363|Suominen Souveniers|2447.76|  0.23333|\n",
      "|      10363|Suominen Souveniers|2632.89|  0.26667|\n",
      "|      10363|Suominen Souveniers| 2773.8|      0.3|\n",
      "|      10363|Suominen Souveniers|2775.08|  0.33333|\n",
      "|      10363|Suominen Souveniers|2817.87|  0.36667|\n",
      "|      10363|Suominen Souveniers|2851.84|      0.4|\n",
      "|      10363|Suominen Souveniers|2931.98|  0.43333|\n",
      "|      10247|Suominen Souveniers|3128.65|  0.46667|\n",
      "|      10363|Suominen Souveniers|3288.82|      0.5|\n",
      "|      10363|Suominen Souveniers|3595.62|  0.53333|\n",
      "|      10363|Suominen Souveniers|3686.54|  0.56667|\n",
      "|      10141|Suominen Souveniers| 3784.8|      0.6|\n",
      "|      10363|Suominen Souveniers| 4068.7|  0.63333|\n",
      "|      10363|Suominen Souveniers|4142.64|  0.66667|\n",
      "|      10247|Suominen Souveniers|4157.73|      0.7|\n",
      "|      10247|Suominen Souveniers|4381.25|  0.73333|\n",
      "|      10141|Suominen Souveniers| 4836.5|  0.76667|\n",
      "|      10363|Suominen Souveniers|5154.41|      0.8|\n",
      "|      10141|Suominen Souveniers|5500.44|  0.83333|\n",
      "|      10141|Suominen Souveniers|5938.53|  0.86667|\n",
      "|      10141|Suominen Souveniers|6287.66|      0.9|\n",
      "|      10363|Suominen Souveniers| 6576.5|  0.93333|\n",
      "|      10247|Suominen Souveniers| 6756.0|  0.96667|\n",
      "|      10247|Suominen Souveniers|10606.2|      1.0|\n",
      "|      10280| Amica Models & Co.|  577.6|  0.03846|\n",
      "|      10280| Amica Models & Co.|1381.05|  0.07692|\n",
      "|      10280| Amica Models & Co.|1557.36|  0.11538|\n",
      "|      10280| Amica Models & Co.| 1574.0|  0.15385|\n",
      "|      10280| Amica Models & Co.|1656.69|  0.19231|\n",
      "|      10293| Amica Models & Co.|1921.92|  0.23077|\n",
      "|      10293| Amica Models & Co.|2084.81|  0.26923|\n",
      "|      10280| Amica Models & Co.|2137.05|  0.30769|\n",
      "|      10293| Amica Models & Co.|2418.24|  0.34615|\n",
      "|      10280| Amica Models & Co.|2800.08|  0.38462|\n",
      "|      10293| Amica Models & Co.|2819.28|  0.42308|\n",
      "|      10293| Amica Models & Co.|2941.89|  0.46154|\n",
      "|      10280| Amica Models & Co.|2954.53|      0.5|\n",
      "|      10280| Amica Models & Co.|3006.43|  0.53846|\n",
      "|      10280| Amica Models & Co.|3474.46|  0.57692|\n",
      "|      10280| Amica Models & Co.| 3668.6|  0.61538|\n",
      "|      10280| Amica Models & Co.|3704.05|  0.65385|\n",
      "|      10293| Amica Models & Co.|4242.24|  0.69231|\n",
      "|      10280| Amica Models & Co.| 4455.0|  0.73077|\n",
      "|      10280| Amica Models & Co.| 4750.8|  0.76923|\n",
      "+-----------+-------------------+-------+---------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions\n",
    "\n",
    "#sales_df1.groupBy(['CUSTOMERNAME'])\\\n",
    "#         .agg(\n",
    "#            round(sum('SALES'), 3).alias('SUM'),\n",
    "#            round(avg('SALES'), 3).alias('MEAN'),\n",
    "#            round(expr(f\"percentile_approx({'SALES'}, 0.5)\"), 3).alias('MEDIAN'),\n",
    "#            round(stddev('SALES'), 3).alias('STDDEV'),\n",
    "#            round(count('SALES'), 3).alias('COUNT'),\n",
    "#            round(max('SALES'), 3).alias('MAX'),\n",
    "#            round(min('SALEs'), 3).alias('MIN')\n",
    "#         )\\\n",
    "#         .show()\n",
    "\n",
    "from pyspark.sql.functions import cume_dist\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "sales_df1\\\n",
    ".withColumn(\"cume_dist\", round(cume_dist().over(Window.partitionBy('CUSTOMERNAME').orderBy(\"SALES\")), 5))\\\n",
    ".select(\"ORDERNUMBER\", \"CUSTOMERNAME\", \"SALES\", \"cume_dist\")\\\n",
    ".show(50)\n",
    "\n",
    "# INSIGHT: for the first row, cume_dist is 0.033 means that for the particular customer, \n",
    "# SALES value of that order is higher than 3.3% of all SALES values of the customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d791206",
   "metadata": {},
   "source": [
    "### o) Write spark dataframe code to rank products by total revenue within each country (COUNTRY)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48f6b4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------------+----+\n",
      "|    COUNTRY|     PRODUCTLINE|TOTALREVENUE|rank|\n",
      "+-----------+----------------+------------+----+\n",
      "|     Sweden|    Classic Cars|    50377.62|   1|\n",
      "|     Sweden|Trucks and Buses|    39562.44|   2|\n",
      "|     Sweden|    Vintage Cars|    31784.94|   3|\n",
      "|     Sweden|           Ships|    29514.62|   4|\n",
      "|     Sweden|     Motorcycles|     12388.6|   5|\n",
      "|     Sweden|          Planes|     7435.88|   6|\n",
      "|     Sweden|          Trains|      3200.0|   7|\n",
      "|Philippines|    Classic Cars|    43815.85|   1|\n",
      "|Philippines|     Motorcycles|     17491.9|   2|\n",
      "|Philippines|          Planes|    17048.33|   3|\n",
      "|Philippines|    Vintage Cars|     1935.09|   4|\n",
      "|  Singapore|    Classic Cars|    91791.76|   1|\n",
      "|  Singapore|Trucks and Buses|    75797.19|   2|\n",
      "|  Singapore|    Vintage Cars|     30221.0|   3|\n",
      "|  Singapore|           Ships|    13065.74|   4|\n",
      "|  Singapore|          Trains|    12934.21|   5|\n",
      "|  Singapore|     Motorcycles|      4175.6|   6|\n",
      "|    Germany|    Classic Cars|   113357.71|   1|\n",
      "|    Germany|          Planes|    20786.78|   2|\n",
      "|    Germany|    Vintage Cars|    18480.53|   3|\n",
      "+-----------+----------------+------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "\n",
    "sales_df1.withColumn(\n",
    "    'REVENUESUM',col('QUANTITYORDERED')*col('PRICEEACH')\n",
    ")\\\n",
    ".groupBy(['COUNTRY', 'PRODUCTLINE']).agg(round(sum('REVENUESUM'), 3).alias('TOTALREVENUE'))\\\n",
    ".withColumn('rank', dense_rank().over(Window.partitionBy('COUNTRY').orderBy(col('TOTALREVENUE').desc())))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf58e79",
   "metadata": {},
   "source": [
    "### p) Calculate a running total of SALES for each customer and show the top 5 customers by this cumulative total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "160d41dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+\n",
      "|        CUSTOMERNAME|TOTALSALES|    cumulative_sum|\n",
      "+--------------------+----------+------------------+\n",
      "|Euro Shopping Cha...| 912294.11|     1.003262885E7|\n",
      "|Mini Gifts Distri...| 654858.06|        9120334.74|\n",
      "|Australian Collec...| 200995.41|        8465476.68|\n",
      "|  Muscle Machine Inc| 197736.94|8264481.2700000005|\n",
      "|   La Rochelle Gifts|  180124.9|        8066744.33|\n",
      "+--------------------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "sales_df1\\\n",
    ".groupBy(['CUSTOMERNAME'])\\\n",
    ".agg(round(sum('SALES'), 3).alias('TOTALSALES'))\\\n",
    ".withColumn(\n",
    "        'cumulative_sum',\n",
    "        sum('TOTALSALES')\\\n",
    "            .over(Window.orderBy('TOTALSALES')\\\n",
    "                .rowsBetween(Window.unboundedPreceding, Window.currentRow))\n",
    ").orderBy('cumulative_sum', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976dd6f9",
   "metadata": {},
   "source": [
    "### q) Find and handle Invalid and Outliers values in entire DataFrame. (Check for only continuous dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50d90590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+\n",
      "|ORDERNUMBER|QUANTITYORDERED|PRICEEACH|ORDERLINENUMBER|  SALES|      ORDERDATE| STATUS|QTR_ID|MONTH_ID|YEAR_ID|PRODUCTLINE|MSRP|PRODUCTCODE|        CUSTOMERNAME|           PHONE|        ADDRESSLINE1|ADDRESSLINE2|         CITY|   STATE|POSTALCODE|  COUNTRY|TERRITORY|CONTACTLASTNAME|CONTACTFIRSTNAME|DEALSIZE|\n",
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+\n",
      "|      10107|             30|     95.7|              2| 2871.0| 2/24/2003 0:00|Shipped|     1|       2|   2003|Motorcycles|  95|   S10_1678|   Land of Toys Inc.|      2125557818|897 Long Airport ...|            |          NYC|      NY|     10022|      USA|       NA|             Yu|            Kwai|   Small|\n",
      "|      10121|             34|    81.35|              5| 2765.9|  5/7/2003 0:00|Shipped|     2|       5|   2003|Motorcycles|  95|   S10_1678|  Reims Collectables|      26.47.1555|  59 rue de l'Abbaye|            |        Reims|        |     51100|   France|     EMEA|        Henriot|            Paul|   Small|\n",
      "|      10134|             41|    94.74|              2|3884.34|  7/1/2003 0:00|Shipped|     3|       7|   2003|Motorcycles|  95|   S10_1678|     Lyon Souveniers|+33 1 46 62 7555|27 rue du Colonel...|            |        Paris|        |     75508|   France|     EMEA|       Da Cunha|          Daniel|  Medium|\n",
      "|      10145|             45|    83.26|              6| 3746.7| 8/25/2003 0:00|Shipped|     3|       8|   2003|Motorcycles|  95|   S10_1678|   Toys4GrownUps.com|      6265557265|  78934 Hillside Dr.|            |     Pasadena|      CA|     90003|      USA|       NA|          Young|           Julie|  Medium|\n",
      "|      10159|             49|    100.0|             14|5205.27|10/10/2003 0:00|Shipped|     4|      10|   2003|Motorcycles|  95|   S10_1678|Corporate Gift Id...|      6505551386|     7734 Strong St.|            |San Francisco|      CA|          |      USA|       NA|          Brown|           Julie|  Medium|\n",
      "|      10168|             36|    96.66|              1|3479.76|10/28/2003 0:00|Shipped|     4|      10|   2003|Motorcycles|  95|   S10_1678|Technics Stores Inc.|      6505556809|   9408 Furth Circle|            |   Burlingame|      CA|     94217|      USA|       NA|         Hirano|            Juri|  Medium|\n",
      "|      10180|             29|    86.13|              9|2497.77|11/11/2003 0:00|Shipped|     4|      11|   2003|Motorcycles|  95|   S10_1678|Daedalus Designs ...|      20.16.1555|184, chausse de T...|            |        Lille|        |     59000|   France|     EMEA|          Rance|         Martine|   Small|\n",
      "|      10188|             48|    100.0|              1|5512.32|11/18/2003 0:00|Shipped|     4|      11|   2003|Motorcycles|  95|   S10_1678|        Herkku Gifts|   +47 2267 3215|Drammen 121, PR 7...|            |       Bergen|        |    N 5804|   Norway|     EMEA|         Oeztan|          Veysel|  Medium|\n",
      "|      10201|             22|    98.57|              2|2168.54| 12/1/2003 0:00|Shipped|     4|      12|   2003|Motorcycles|  95|   S10_1678|     Mini Wheels Co.|      6505555787|5557 North Pendal...|            |San Francisco|      CA|          |      USA|       NA|         Murphy|           Julie|   Small|\n",
      "|      10211|             41|    100.0|             14|4708.44| 1/15/2004 0:00|Shipped|     1|       1|   2004|Motorcycles|  95|   S10_1678|    Auto Canal Petit|  (1) 47.55.6555|   25, rue Lauriston|            |        Paris|        |     75016|   France|     EMEA|        Perrier|       Dominique|  Medium|\n",
      "|      10223|             37|    100.0|              1|3965.66| 2/20/2004 0:00|Shipped|     1|       2|   2004|Motorcycles|  95|   S10_1678|Australian Collec...|    03 9520 4555|   636 St Kilda Road|     Level 3|    Melbourne|Victoria|      3004|Australia|     APAC|       Ferguson|           Peter|  Medium|\n",
      "|      10237|             23|    100.0|              7|2333.12|  4/5/2004 0:00|Shipped|     2|       4|   2004|Motorcycles|  95|   S10_1678|     Vitachrome Inc.|      2125551500|   2678 Kingston Rd.|   Suite 101|          NYC|      NY|     10022|      USA|       NA|          Frick|         Michael|   Small|\n",
      "|      10251|             28|    100.0|              2|3188.64| 5/18/2004 0:00|Shipped|     2|       5|   2004|Motorcycles|  95|   S10_1678|Tekni Collectable...|      2015559350|       7476 Moss Rd.|            |       Newark|      NJ|     94019|      USA|       NA|          Brown|         William|  Medium|\n",
      "|      10263|             34|    100.0|              2|3676.76| 6/28/2004 0:00|Shipped|     2|       6|   2004|Motorcycles|  95|   S10_1678|     Gift Depot Inc.|      2035552570| 25593 South Bay Ln.|            |  Bridgewater|      CT|     97562|      USA|       NA|           King|           Julie|  Medium|\n",
      "|      10275|             45|    92.83|              1|4177.35| 7/23/2004 0:00|Shipped|     3|       7|   2004|Motorcycles|  95|   S10_1678|   La Rochelle Gifts|      40.67.8555|67, rue des Cinqu...|            |       Nantes|        |     44000|   France|     EMEA|        Labrune|          Janine|  Medium|\n",
      "|      10285|             36|    100.0|              6|4099.68| 8/27/2004 0:00|Shipped|     3|       8|   2004|Motorcycles|  95|   S10_1678|Marta's Replicas Co.|      6175558555| 39323 Spinnaker Dr.|            |    Cambridge|      MA|     51247|      USA|       NA|      Hernandez|           Marta|  Medium|\n",
      "|      10299|             23|    100.0|              9|2597.39| 9/30/2004 0:00|Shipped|     3|       9|   2004|Motorcycles|  95|   S10_1678|Toys of Finland, Co.|     90-224 8555|       Keskuskatu 45|            |     Helsinki|        |     21240|  Finland|     EMEA|      Karttunen|           Matti|   Small|\n",
      "|      10309|             41|    100.0|              5|4394.38|10/15/2004 0:00|Shipped|     4|      10|   2004|Motorcycles|  95|   S10_1678|  Baane Mini Imports|      07-98 9555|Erling Skakkes ga...|            |      Stavern|        |      4110|   Norway|     EMEA|     Bergulfsen|           Jonas|  Medium|\n",
      "|      10318|             46|    94.74|              1|4358.04| 11/2/2004 0:00|Shipped|     4|      11|   2004|Motorcycles|  95|   S10_1678|Diecast Classics ...|      2155551555|    7586 Pompton St.|            |    Allentown|      PA|     70267|      USA|       NA|             Yu|           Kyung|  Medium|\n",
      "|      10329|             42|    100.0|              1|4396.14|11/15/2004 0:00|Shipped|     4|      11|   2004|Motorcycles|  95|   S10_1678|   Land of Toys Inc.|      2125557818|897 Long Airport ...|            |          NYC|      NY|     10022|      USA|       NA|             Yu|            Kwai|  Medium|\n",
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"ORDERLINENUMBER\", \"ORDERNUMBER\", should be included? [coz not necessarily continuous]\n",
    "col_list = [\"QUANTITYORDERED\", \"PRICEEACH\", \"SALES\", \"QTR_ID\", \"MONTH_ID\", \"YEAR_ID\", \"MSRP\"]\n",
    "\n",
    "\n",
    "# creating a deepcopy for us to work on\n",
    "new_dataframe = sales_df1.alias(\"new_dataframe\")\n",
    "\n",
    "# only from the columns we determined as continuous, drop NA values\n",
    "new_dataframe = new_dataframe.dropna(subset=col_list)\n",
    "\n",
    "# for each filed, filter out values outside threshold\n",
    "for fieldName in col_list:\n",
    "    q1 = new_dataframe.approxQuantile(fieldName, [0.25], relativeError=0.0001)[0]\n",
    "    q3 = new_dataframe.approxQuantile(fieldName, [0.75], relativeError=0.0001)[0]\n",
    "\n",
    "    minThreshold = q1 - 1.5*(q3-q1)\n",
    "    maxThreshold = q3 + 1.5*(q3-q1)\n",
    "    #print(f\"{fieldName}, {minThreshold}, {maxThreshold}\")\n",
    "\n",
    "    new_dataframe = new_dataframe.filter((col(fieldName)>=minThreshold) & (col(fieldName)<=maxThreshold))\n",
    "\n",
    "new_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38163e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1cd40",
   "metadata": {},
   "source": [
    "### r) How would you cache a DataFrame containing sales data from the top 10 countries by sales to avoid recomputation in subsequent transformations? What persistence level (e.g. MEMORY_ONLY, MEMORY_AND_DISK) would you choose and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94a3bead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|  COUNTRY|TOTALSALES|\n",
      "+---------+----------+\n",
      "|      USA|3627982.83|\n",
      "|    Spain|1215686.92|\n",
      "|   France|1110916.52|\n",
      "|Australia|  630623.1|\n",
      "|       UK| 478880.46|\n",
      "|    Italy| 374674.31|\n",
      "|  Finland| 329581.91|\n",
      "|   Norway|  307463.7|\n",
      "|Singapore| 288488.41|\n",
      "|  Denmark| 245637.15|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_top10 = sales_df1.groupBy(['COUNTRY'])\\\n",
    ".agg(round(sum('SALES'), 3).alias(\"TOTALSALES\"))\\\n",
    ".orderBy(col('TOTALSALES').desc())\\\n",
    ".limit(10)\n",
    "\n",
    "countries_top10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8dcb0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caching\n",
    "countries_top10_cached = countries_top10.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ddd8cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (persistence)[https://sparkbyexamples.com/pyspark/pyspark-persist-in-detail/]\n",
    "from pyspark import StorageLevel\n",
    "countries_top10_persisted = countries_top10.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# the size of data that we are smalling here is small [10 rows and 2 columns], which can sit in the memory\n",
    "# MEMORY_ONLY_SER is not used as serialization and deserialization requires overhead in processing\n",
    "# MEMORY_AND_DISK need to be used only when storage required is larger than available memory\n",
    "# replication to more cluster notes is also not required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052f4416",
   "metadata": {},
   "source": [
    "### s) How would you pivot the data to show PRODUCTLINE as columns and the total SALES for each ORDERDATE as the values? What are the implications of pivoting large datasets in Spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b46b379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+--------+--------+-------+----------------+------------+\n",
      "| ORDERDATE|Classic Cars|Motorcycles|  Planes|   Ships| Trains|Trucks and Buses|Vintage Cars|\n",
      "+----------+------------+-----------+--------+--------+-------+----------------+------------+\n",
      "|2003-01-06|        null|       null|    null|    null|   null|            null|    12133.25|\n",
      "|2003-01-09|        null|       null|    null|    null|   null|            null|    11432.34|\n",
      "|2003-01-10|        null|       null|    null|    null|   null|            null|     6864.05|\n",
      "|2003-01-29|     15263.7|       null|    null|    null|   null|         23041.1|     16397.2|\n",
      "|2003-01-31|    25928.08|       null|    null|    null|4933.55|        13760.33|        null|\n",
      "|2003-02-11|    20464.41|       null|    null|20452.04| 4330.1|            null|    13624.56|\n",
      "|2003-02-17|        null|       null|39205.31| 6598.34|   null|            null|    10377.67|\n",
      "|2003-02-24|        null|   25783.76|    null|    null|   null|            null|        null|\n",
      "|2003-03-03|    42605.87|   12639.15|    null|    null|   null|            null|        null|\n",
      "|2003-03-10|    27398.82|       null|    null|    null|   null|            null|        null|\n",
      "|2003-03-18|    27812.88|       null|    null|    null|   null|            null|    23205.04|\n",
      "|2003-03-24|     7209.11|       null|    null|    null|   null|            null|     2539.89|\n",
      "|2003-03-25|        null|       null|    null|    null|   null|            null|    18695.58|\n",
      "|2003-03-26|        null|       null|    null|    null|   null|         9908.06|      2490.5|\n",
      "|2003-04-01|    17994.34|       null|    null|    null|   null|        20223.07|        null|\n",
      "|2003-04-04|    16209.72|       null|    null|    null|   null|         8567.69|        null|\n",
      "|2003-04-11|        null|       null|    null|    null|1711.26|            null|        null|\n",
      "|2003-04-16|    20664.74|       null|    null|14155.52|3045.21|            null|      5792.0|\n",
      "|2003-04-21|        null|       null|    null|  4219.2|   null|            null|        null|\n",
      "|2003-04-28|      5004.8|       null|14216.32| 9024.73|   null|            null|    10383.29|\n",
      "+----------+------------+-----------+--------+--------+-------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df1.withColumn('ORDERDATE', to_timestamp(col('ORDERDATE'), 'MM/dd/yyyy H:mm'))\\\n",
    ".withColumn(\"ORDERDATE\", col(\"ORDERDATE\").cast('date'))\\\n",
    ".groupBy(\"ORDERDATE\")\\\n",
    ".pivot(\"PRODUCTLINE\")\\\n",
    ".agg(round(sum(\"SALES\"), 3))\\\n",
    ".orderBy(\"ORDERDATE\")\\\n",
    ".show()\n",
    "\n",
    "# performing pivot operations on lagre datasets can cause increased memory consumption\n",
    "# and higher processing time.\n",
    "# number of new columns in the new dataframe will depend on unique values in the pivot column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d34fef",
   "metadata": {},
   "source": [
    "### t) How would you calculate the percentage growth of total sales month over month for each PRODUCTLINE using Spark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "353cdd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+---------+---------+-----------------+\n",
      "| PRODUCTLINE|YEAR|MONTH|    SALES|      LAG|PERCENTAGE_GROWTH|\n",
      "+------------+----+-----+---------+---------+-----------------+\n",
      "| Motorcycles|2003|    2| 25783.76|     null|             null|\n",
      "| Motorcycles|2003|    3| 12639.15| 25783.76|           -50.98|\n",
      "| Motorcycles|2003|    4| 23475.59| 12639.15|           85.737|\n",
      "| Motorcycles|2003|    5| 22097.32| 23475.59|           -5.871|\n",
      "| Motorcycles|2003|    6|  2642.01| 22097.32|          -88.044|\n",
      "| Motorcycles|2003|    7| 37924.23|  2642.01|         1335.431|\n",
      "| Motorcycles|2003|    8| 44164.91| 37924.23|           16.456|\n",
      "| Motorcycles|2003|    9|  3155.58| 44164.91|          -92.855|\n",
      "| Motorcycles|2003|   10| 64235.65|  3155.58|         1935.621|\n",
      "| Motorcycles|2003|   11| 109345.5| 64235.65|           70.226|\n",
      "| Motorcycles|2003|   12| 25431.88| 109345.5|          -76.742|\n",
      "| Motorcycles|2004|    1| 41200.52| 25431.88|           62.003|\n",
      "| Motorcycles|2004|    2|  49066.5| 41200.52|           19.092|\n",
      "| Motorcycles|2004|    4| 36269.07|  49066.5|          -26.082|\n",
      "| Motorcycles|2004|    5| 46848.95| 36269.07|           29.171|\n",
      "| Motorcycles|2004|    6| 47237.41| 46848.95|            0.829|\n",
      "| Motorcycles|2004|    7|  22774.0| 47237.41|          -51.788|\n",
      "| Motorcycles|2004|    8| 62704.93|  22774.0|          175.336|\n",
      "| Motorcycles|2004|    9| 42471.05| 62704.93|          -32.268|\n",
      "| Motorcycles|2004|   10| 39413.96| 42471.05|           -7.198|\n",
      "| Motorcycles|2004|   11|151711.86| 39413.96|          284.919|\n",
      "| Motorcycles|2004|   12| 20846.98|151711.86|          -86.259|\n",
      "| Motorcycles|2005|    1| 39913.36| 20846.98|           91.459|\n",
      "| Motorcycles|2005|    2| 47951.42| 39913.36|           20.139|\n",
      "| Motorcycles|2005|    3| 47830.83| 47951.42|           -0.251|\n",
      "| Motorcycles|2005|    4| 59862.22| 47830.83|           25.154|\n",
      "| Motorcycles|2005|    5|  39389.7| 59862.22|          -34.199|\n",
      "|Vintage Cars|2003|    1| 46826.84|     null|             null|\n",
      "|Vintage Cars|2003|    2| 24002.23| 46826.84|          -48.743|\n",
      "|Vintage Cars|2003|    3| 46931.01| 24002.23|           95.528|\n",
      "|Vintage Cars|2003|    4| 20750.34| 46931.01|          -55.785|\n",
      "|Vintage Cars|2003|    5| 47033.58| 20750.34|          126.664|\n",
      "|Vintage Cars|2003|    6| 26582.51| 47033.58|          -43.482|\n",
      "|Vintage Cars|2003|    7| 29652.09| 26582.51|           11.547|\n",
      "|Vintage Cars|2003|    8| 23285.46| 29652.09|          -21.471|\n",
      "+------------+----+-----+---------+---------+-----------------+\n",
      "only showing top 35 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "sales_df1\\\n",
    ".withColumn('ORDERDATE', to_timestamp(col('ORDERDATE'), 'MM/dd/yyyy H:mm'))\\\n",
    ".select(\n",
    "    col('PRODUCTLINE'), year('ORDERDATE').alias('YEAR'), month('ORDERDATE').alias('MONTH'), col('SALES'))\\\n",
    ".groupBy([\"PRODUCTLINE\", \"YEAR\", \"MONTH\"]).agg(round(sum(\"SALES\"), 3).alias(\"SALES\"))\\\n",
    ".withColumn(\n",
    "    'LAG', \n",
    "    lag('SALES', 1).over(Window.partitionBy(\"PRODUCTLINE\").orderBy(\"PRODUCTLINE\", \"YEAR\", \"MONTH\")))\\\n",
    ".withColumn(\n",
    "    'PERCENTAGE_GROWTH', \n",
    "    round((col(\"SALES\")-col(\"LAG\"))*100/col(\"LAG\"), 3).alias('PERCENTAGE_GROWTH'))\\\n",
    ".show(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61019b",
   "metadata": {},
   "source": [
    "### u) How can you rebalance the data by partioning based on the COUNTRY column to ensure that large data partitions are avoided?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cf4bbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df1_repartitioned = sales_df1.repartition(col(\"COUNTRY\"))\n",
    "sales_df1_repartitioned.rdd.getNumPartitions()\n",
    "\n",
    "# repartition() helps distribute the dataframe across different partitions, thereby balancing\n",
    "# the distribution of data. This helps improve parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03748e58",
   "metadata": {},
   "source": [
    "### v) Suppose you have a smaller lookup table with customer details. How would you perform a broadcast join with the large sales_data_sample dataset to improve join performance? What are the key considerations when using broadcast joins?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b5d750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+--------------------+---------+-------------+\n",
      "|ORDERNUMBER|QUANTITYORDERED|PRICEEACH|ORDERLINENUMBER|  SALES|      ORDERDATE| STATUS|QTR_ID|MONTH_ID|YEAR_ID|PRODUCTLINE|MSRP|PRODUCTCODE|        CUSTOMERNAME|           PHONE|        ADDRESSLINE1|ADDRESSLINE2|         CITY|   STATE|POSTALCODE|  COUNTRY|TERRITORY|CONTACTLASTNAME|CONTACTFIRSTNAME|DEALSIZE|        CUSTOMERNAME|ESTD_YEAR|NUM_EMPLOYEES|\n",
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+--------------------+---------+-------------+\n",
      "|      10107|             30|     95.7|              2| 2871.0| 2/24/2003 0:00|Shipped|     1|       2|   2003|Motorcycles|  95|   S10_1678|   Land of Toys Inc.|      2125557818|897 Long Airport ...|            |          NYC|      NY|     10022|      USA|       NA|             Yu|            Kwai|   Small|   Land of Toys Inc.|     2009|           75|\n",
      "|      10121|             34|    81.35|              5| 2765.9|  5/7/2003 0:00|Shipped|     2|       5|   2003|Motorcycles|  95|   S10_1678|  Reims Collectables|      26.47.1555|  59 rue de l'Abbaye|            |        Reims|        |     51100|   France|     EMEA|        Henriot|            Paul|   Small|  Reims Collectables|     2015|           70|\n",
      "|      10134|             41|    94.74|              2|3884.34|  7/1/2003 0:00|Shipped|     3|       7|   2003|Motorcycles|  95|   S10_1678|     Lyon Souveniers|+33 1 46 62 7555|27 rue du Colonel...|            |        Paris|        |     75508|   France|     EMEA|       Da Cunha|          Daniel|  Medium|     Lyon Souveniers|     1995|           60|\n",
      "|      10145|             45|    83.26|              6| 3746.7| 8/25/2003 0:00|Shipped|     3|       8|   2003|Motorcycles|  95|   S10_1678|   Toys4GrownUps.com|      6265557265|  78934 Hillside Dr.|            |     Pasadena|      CA|     90003|      USA|       NA|          Young|           Julie|  Medium|   Toys4GrownUps.com|     2008|          120|\n",
      "|      10159|             49|    100.0|             14|5205.27|10/10/2003 0:00|Shipped|     4|      10|   2003|Motorcycles|  95|   S10_1678|Corporate Gift Id...|      6505551386|     7734 Strong St.|            |San Francisco|      CA|          |      USA|       NA|          Brown|           Julie|  Medium|Corporate Gift Id...|     1999|           50|\n",
      "|      10168|             36|    96.66|              1|3479.76|10/28/2003 0:00|Shipped|     4|      10|   2003|Motorcycles|  95|   S10_1678|Technics Stores Inc.|      6505556809|   9408 Furth Circle|            |   Burlingame|      CA|     94217|      USA|       NA|         Hirano|            Juri|  Medium|Technics Stores Inc.|     2010|           80|\n",
      "|      10180|             29|    86.13|              9|2497.77|11/11/2003 0:00|Shipped|     4|      11|   2003|Motorcycles|  95|   S10_1678|Daedalus Designs ...|      20.16.1555|184, chausse de T...|            |        Lille|        |     59000|   France|     EMEA|          Rance|         Martine|   Small|Daedalus Designs ...|     2002|          100|\n",
      "|      10188|             48|    100.0|              1|5512.32|11/18/2003 0:00|Shipped|     4|      11|   2003|Motorcycles|  95|   S10_1678|        Herkku Gifts|   +47 2267 3215|Drammen 121, PR 7...|            |       Bergen|        |    N 5804|   Norway|     EMEA|         Oeztan|          Veysel|  Medium|        Herkku Gifts|     2000|           90|\n",
      "|      10201|             22|    98.57|              2|2168.54| 12/1/2003 0:00|Shipped|     4|      12|   2003|Motorcycles|  95|   S10_1678|     Mini Wheels Co.|      6505555787|5557 North Pendal...|            |San Francisco|      CA|          |      USA|       NA|         Murphy|           Julie|   Small|     Mini Wheels Co.|     2009|           35|\n",
      "|      10211|             41|    100.0|             14|4708.44| 1/15/2004 0:00|Shipped|     1|       1|   2004|Motorcycles|  95|   S10_1678|    Auto Canal Petit|  (1) 47.55.6555|   25, rue Lauriston|            |        Paris|        |     75016|   France|     EMEA|        Perrier|       Dominique|  Medium|    Auto Canal Petit|     2002|           55|\n",
      "|      10223|             37|    100.0|              1|3965.66| 2/20/2004 0:00|Shipped|     1|       2|   2004|Motorcycles|  95|   S10_1678|Australian Collec...|    03 9520 4555|   636 St Kilda Road|     Level 3|    Melbourne|Victoria|      3004|Australia|     APAC|       Ferguson|           Peter|  Medium|Australian Collec...|     2007|           60|\n",
      "|      10237|             23|    100.0|              7|2333.12|  4/5/2004 0:00|Shipped|     2|       4|   2004|Motorcycles|  95|   S10_1678|     Vitachrome Inc.|      2125551500|   2678 Kingston Rd.|   Suite 101|          NYC|      NY|     10022|      USA|       NA|          Frick|         Michael|   Small|     Vitachrome Inc.|     2015|           25|\n",
      "|      10251|             28|    100.0|              2|3188.64| 5/18/2004 0:00|Shipped|     2|       5|   2004|Motorcycles|  95|   S10_1678|Tekni Collectable...|      2015559350|       7476 Moss Rd.|            |       Newark|      NJ|     94019|      USA|       NA|          Brown|         William|  Medium|Tekni Collectable...|     2007|           85|\n",
      "|      10263|             34|    100.0|              2|3676.76| 6/28/2004 0:00|Shipped|     2|       6|   2004|Motorcycles|  95|   S10_1678|     Gift Depot Inc.|      2035552570| 25593 South Bay Ln.|            |  Bridgewater|      CT|     97562|      USA|       NA|           King|           Julie|  Medium|     Gift Depot Inc.|     2002|           80|\n",
      "|      10275|             45|    92.83|              1|4177.35| 7/23/2004 0:00|Shipped|     3|       7|   2004|Motorcycles|  95|   S10_1678|   La Rochelle Gifts|      40.67.8555|67, rue des Cinqu...|            |       Nantes|        |     44000|   France|     EMEA|        Labrune|          Janine|  Medium|   La Rochelle Gifts|     1999|           55|\n",
      "|      10285|             36|    100.0|              6|4099.68| 8/27/2004 0:00|Shipped|     3|       8|   2004|Motorcycles|  95|   S10_1678|Marta's Replicas Co.|      6175558555| 39323 Spinnaker Dr.|            |    Cambridge|      MA|     51247|      USA|       NA|      Hernandez|           Marta|  Medium|Marta's Replicas Co.|     2009|           45|\n",
      "|      10299|             23|    100.0|              9|2597.39| 9/30/2004 0:00|Shipped|     3|       9|   2004|Motorcycles|  95|   S10_1678|Toys of Finland, Co.|     90-224 8555|       Keskuskatu 45|            |     Helsinki|        |     21240|  Finland|     EMEA|      Karttunen|           Matti|   Small|Toys of Finland, Co.|     2014|           50|\n",
      "|      10309|             41|    100.0|              5|4394.38|10/15/2004 0:00|Shipped|     4|      10|   2004|Motorcycles|  95|   S10_1678|  Baane Mini Imports|      07-98 9555|Erling Skakkes ga...|            |      Stavern|        |      4110|   Norway|     EMEA|     Bergulfsen|           Jonas|  Medium|  Baane Mini Imports|     1997|           60|\n",
      "|      10318|             46|    94.74|              1|4358.04| 11/2/2004 0:00|Shipped|     4|      11|   2004|Motorcycles|  95|   S10_1678|Diecast Classics ...|      2155551555|    7586 Pompton St.|            |    Allentown|      PA|     70267|      USA|       NA|             Yu|           Kyung|  Medium|Diecast Classics ...|     2006|           65|\n",
      "|      10329|             42|    100.0|              1|4396.14|11/15/2004 0:00|Shipped|     4|      11|   2004|Motorcycles|  95|   S10_1678|   Land of Toys Inc.|      2125557818|897 Long Airport ...|            |          NYC|      NY|     10022|      USA|       NA|             Yu|            Kwai|  Medium|   Land of Toys Inc.|     2009|           75|\n",
      "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+--------------------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "\n",
    "customer_lookup_table = spark.read.csv('file:///home/hadoop/Downloads/sales_data_customer_demographic.csv', header=True, inferSchema=True)\n",
    "\n",
    "joined_df = sales_df1.join(broadcast(customer_lookup_table), sales_df1.CUSTOMERNAME==customer_lookup_table.CUSTOMERNAME, \"inner\")\n",
    "joined_df.show()\n",
    "\n",
    "\n",
    "# key considerations when using broadcast join\n",
    "# - one of the dataframes must be small enough to fit in the memory of each worker node\n",
    "# it makes the join operation much less resource intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d0e4a",
   "metadata": {},
   "source": [
    "### w) Create a UDF that categorizes the sales values (SALES) into custom buckets like Low, Medium, High. Apply this UDF to the DataFrame and calculate the count of orders in each category per COUNTRY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7236110d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---+------+\n",
      "|    COUNTRY|High|Low|Medium|\n",
      "+-----------+----+---+------+\n",
      "|  Australia|  61| 67|    57|\n",
      "|    Austria|  20| 16|    19|\n",
      "|    Belgium|  11| 14|     8|\n",
      "|     Canada|  18| 28|    24|\n",
      "|    Denmark|  23| 18|    22|\n",
      "|    Finland|  32| 25|    35|\n",
      "|     France| 102|112|   100|\n",
      "|    Germany|  21| 22|    19|\n",
      "|    Ireland|   8|  7|     1|\n",
      "|      Italy|  32| 38|    43|\n",
      "|      Japan|  15| 16|    21|\n",
      "|     Norway|  34| 30|    21|\n",
      "|Philippines|   9|  7|    10|\n",
      "|  Singapore|  30| 28|    21|\n",
      "|      Spain| 119|112|   111|\n",
      "|     Sweden|  20| 16|    21|\n",
      "|Switzerland|  14|  5|    12|\n",
      "|         UK|  36| 53|    55|\n",
      "|        USA| 354|318|   332|\n",
      "+-----------+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "q33 = sales_df1.approxQuantile(\"SALES\", [0.33], relativeError=0.0001)[0]\n",
    "q66 = sales_df1.approxQuantile(\"SALES\", [0.66], relativeError=0.0001)[0]\n",
    "\n",
    "def categorizeSales(sales):\n",
    "    if sales <= q33:\n",
    "        return 'Low'\n",
    "    elif sales <= q66:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "# register UDF\n",
    "udf_categorizeSales = udf(categorizeSales, StringType())\n",
    "\n",
    "sales_df1.withColumn(\"SALES_CATEGORY\", udf_categorizeSales(col(\"SALES\")))\\\n",
    ".groupBy(\"COUNTRY\")\\\n",
    ".pivot(\"SALES_CATEGORY\")\\\n",
    ".agg(count(\"SALES_CATEGORY\"))\\\n",
    ".orderBy(\"COUNTRY\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873e845",
   "metadata": {},
   "source": [
    "### x) Create a Python UDF to calculate discounts for specific product lines. For example, give a 10% discount for Classic Cars and 5% for Motorcycles. Apply this UDF to derive new discounted sales values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d444c296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+-------+--------------+----------------+\n",
      "|ORDERNUMBER|        CUSTOMERNAME|PRODUCTLINE|  SALES|SALES_DISCOUNT|DISCOUNTED_SALES|\n",
      "+-----------+--------------------+-----------+-------+--------------+----------------+\n",
      "|      10107|   Land of Toys Inc.|Motorcycles| 2871.0|          0.05|         2727.45|\n",
      "|      10121|  Reims Collectables|Motorcycles| 2765.9|          0.05|        2627.605|\n",
      "|      10134|     Lyon Souveniers|Motorcycles|3884.34|          0.05|        3690.123|\n",
      "|      10145|   Toys4GrownUps.com|Motorcycles| 3746.7|          0.05|        3559.365|\n",
      "|      10159|Corporate Gift Id...|Motorcycles|5205.27|          0.05|        4945.007|\n",
      "|      10168|Technics Stores Inc.|Motorcycles|3479.76|          0.05|        3305.772|\n",
      "|      10180|Daedalus Designs ...|Motorcycles|2497.77|          0.05|        2372.882|\n",
      "|      10188|        Herkku Gifts|Motorcycles|5512.32|          0.05|        5236.704|\n",
      "|      10201|     Mini Wheels Co.|Motorcycles|2168.54|          0.05|        2060.113|\n",
      "|      10211|    Auto Canal Petit|Motorcycles|4708.44|          0.05|        4473.018|\n",
      "|      10223|Australian Collec...|Motorcycles|3965.66|          0.05|        3767.377|\n",
      "|      10237|     Vitachrome Inc.|Motorcycles|2333.12|          0.05|        2216.464|\n",
      "|      10251|Tekni Collectable...|Motorcycles|3188.64|          0.05|        3029.208|\n",
      "|      10263|     Gift Depot Inc.|Motorcycles|3676.76|          0.05|        3492.922|\n",
      "|      10275|   La Rochelle Gifts|Motorcycles|4177.35|          0.05|        3968.483|\n",
      "|      10285|Marta's Replicas Co.|Motorcycles|4099.68|          0.05|        3894.696|\n",
      "|      10299|Toys of Finland, Co.|Motorcycles|2597.39|          0.05|         2467.52|\n",
      "|      10309|  Baane Mini Imports|Motorcycles|4394.38|          0.05|        4174.661|\n",
      "|      10318|Diecast Classics ...|Motorcycles|4358.04|          0.05|        4140.138|\n",
      "|      10329|   Land of Toys Inc.|Motorcycles|4396.14|          0.05|        4176.333|\n",
      "+-----------+--------------------+-----------+-------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "def discountSales(productLine):\n",
    "    productLine = productLine.lower()\n",
    "    if productLine == 'classic cars': return 0.10\n",
    "    elif productLine == 'motorcycles': return 0.05\n",
    "    elif productLine == 'planes': return 0.20\n",
    "    elif productLine == 'ships': return 0.25\n",
    "    elif productLine == 'trains': return 0.30\n",
    "    elif productLine == 'trucks and buses': return 0.35\n",
    "    elif productLine == 'vintage cars': return 0.40\n",
    "    else: return 0.0\n",
    "\n",
    "# register UDF\n",
    "udf_discountSales = udf(discountSales, DoubleType())\n",
    "\n",
    "sales_df1.withColumn(\"SALES_DISCOUNT\", udf_discountSales(col(\"PRODUCTLINE\")))\\\n",
    ".select(\"ORDERNUMBER\", \"CUSTOMERNAME\", \"PRODUCTLINE\", \"SALES\", \"SALES_DISCOUNT\")\\\n",
    ".withColumn(\"DISCOUNTED_SALES\", round(col(\"SALES\")*(1-col(\"SALES_DISCOUNT\")), 3))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cc87d2",
   "metadata": {},
   "source": [
    "### y) How would you set up an incremental loading mechanism for orders placed daily based on the ORDERDATE column? How can Spark checkpointing can be used with incremental load to ensure no data loss occurs during failures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e425e",
   "metadata": {},
   "source": [
    "inorder to use incremental loading mechanism for processing orders placed daily (assuming data is sorted in order of ORDERDATE), we first need to keep track of the date till which order details have already been loaded. \n",
    "when there is new data with ORDERDATE greater than the tracked date, they will also need to be loaded and tracked date updated.\n",
    "before an update is done, it is necessary to checkpoint the previous state. this is done inorder to ensure that a recovery operation can be performed incase of any issue with the newly updated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1e2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to store checkpoints\n",
    "spark.sparkContext.setCheckpointDir('/home/hadoop/Downloads/checkpoints')\n",
    "\n",
    "# incrementally loading data\n",
    "def loadData(old_df):\n",
    "    new_df = spark.read.csv('file:///home/hadoop/Downloads/sales_data_sample.csv', header=True, inferSchema=True)\n",
    "    updated_df = old_df.union(new_df)\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37122aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    new_sales_df = loadData(sales_df1)\n",
    "    new_sales_df.checkpoint()\n",
    "except Except"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5e1c9",
   "metadata": {},
   "source": [
    "### z) How do you implement a cumulative distribution function (CDF) over the SALES value for each CUSTOMERNAME? What insights can you gather from analyzing the CDF distribution for each customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8dc7c096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------+---------+\n",
      "|ORDERNUMBER|       CUSTOMERNAME|  SALES|cume_dist|\n",
      "+-----------+-------------------+-------+---------+\n",
      "|      10141|Suominen Souveniers| 891.03|  0.03333|\n",
      "|      10141|Suominen Souveniers| 1086.6|  0.06667|\n",
      "|      10141|Suominen Souveniers|1103.76|      0.1|\n",
      "|      10363|Suominen Souveniers|1629.04|  0.13333|\n",
      "|      10247|Suominen Souveniers| 1988.4|  0.16667|\n",
      "|      10141|Suominen Souveniers|2140.11|      0.2|\n",
      "|      10363|Suominen Souveniers|2447.76|  0.23333|\n",
      "|      10363|Suominen Souveniers|2632.89|  0.26667|\n",
      "|      10363|Suominen Souveniers| 2773.8|      0.3|\n",
      "|      10363|Suominen Souveniers|2775.08|  0.33333|\n",
      "|      10363|Suominen Souveniers|2817.87|  0.36667|\n",
      "|      10363|Suominen Souveniers|2851.84|      0.4|\n",
      "|      10363|Suominen Souveniers|2931.98|  0.43333|\n",
      "|      10247|Suominen Souveniers|3128.65|  0.46667|\n",
      "|      10363|Suominen Souveniers|3288.82|      0.5|\n",
      "|      10363|Suominen Souveniers|3595.62|  0.53333|\n",
      "|      10363|Suominen Souveniers|3686.54|  0.56667|\n",
      "|      10141|Suominen Souveniers| 3784.8|      0.6|\n",
      "|      10363|Suominen Souveniers| 4068.7|  0.63333|\n",
      "|      10363|Suominen Souveniers|4142.64|  0.66667|\n",
      "|      10247|Suominen Souveniers|4157.73|      0.7|\n",
      "|      10247|Suominen Souveniers|4381.25|  0.73333|\n",
      "|      10141|Suominen Souveniers| 4836.5|  0.76667|\n",
      "|      10363|Suominen Souveniers|5154.41|      0.8|\n",
      "|      10141|Suominen Souveniers|5500.44|  0.83333|\n",
      "|      10141|Suominen Souveniers|5938.53|  0.86667|\n",
      "|      10141|Suominen Souveniers|6287.66|      0.9|\n",
      "|      10363|Suominen Souveniers| 6576.5|  0.93333|\n",
      "|      10247|Suominen Souveniers| 6756.0|  0.96667|\n",
      "|      10247|Suominen Souveniers|10606.2|      1.0|\n",
      "|      10280| Amica Models & Co.|  577.6|  0.03846|\n",
      "|      10280| Amica Models & Co.|1381.05|  0.07692|\n",
      "|      10280| Amica Models & Co.|1557.36|  0.11538|\n",
      "|      10280| Amica Models & Co.| 1574.0|  0.15385|\n",
      "|      10280| Amica Models & Co.|1656.69|  0.19231|\n",
      "|      10293| Amica Models & Co.|1921.92|  0.23077|\n",
      "|      10293| Amica Models & Co.|2084.81|  0.26923|\n",
      "|      10280| Amica Models & Co.|2137.05|  0.30769|\n",
      "|      10293| Amica Models & Co.|2418.24|  0.34615|\n",
      "|      10280| Amica Models & Co.|2800.08|  0.38462|\n",
      "|      10293| Amica Models & Co.|2819.28|  0.42308|\n",
      "|      10293| Amica Models & Co.|2941.89|  0.46154|\n",
      "|      10280| Amica Models & Co.|2954.53|      0.5|\n",
      "|      10280| Amica Models & Co.|3006.43|  0.53846|\n",
      "|      10280| Amica Models & Co.|3474.46|  0.57692|\n",
      "|      10280| Amica Models & Co.| 3668.6|  0.61538|\n",
      "|      10280| Amica Models & Co.|3704.05|  0.65385|\n",
      "|      10293| Amica Models & Co.|4242.24|  0.69231|\n",
      "|      10280| Amica Models & Co.| 4455.0|  0.73077|\n",
      "|      10280| Amica Models & Co.| 4750.8|  0.76923|\n",
      "+-----------+-------------------+-------+---------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cume_dist\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "sales_df1\\\n",
    ".withColumn(\"cume_dist\", round(cume_dist().over(Window.partitionBy('CUSTOMERNAME').orderBy(\"SALES\")), 5))\\\n",
    ".select(\"ORDERNUMBER\", \"CUSTOMERNAME\", \"SALES\", \"cume_dist\")\\\n",
    ".show(50)\n",
    "\n",
    "# INSIGHT: for the first row, cume_dist is 0.033 means that for the particular customer, \n",
    "# SALES value of that order is higher than 3.3% of all SALES values of the customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19169909",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
