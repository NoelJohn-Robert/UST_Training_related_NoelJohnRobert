{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce22a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c92070e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4bff74b470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f97167",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_employee = spark.read.csv('file:///home/hadoop/Downloads/HR_Employee.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3e3fe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EmployeeID: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- JobRole: string (nullable = true)\n",
      " |-- Attrition: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- MaritalStatus: string (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- EducationField: string (nullable = true)\n",
      " |-- BusinessTravel: string (nullable = true)\n",
      " |-- JobInvolvement: string (nullable = true)\n",
      " |-- JobLevel: integer (nullable = true)\n",
      " |-- JobSatisfaction: string (nullable = true)\n",
      " |-- Hourlyrate: integer (nullable = true)\n",
      " |-- Income: integer (nullable = true)\n",
      " |-- Salaryhike: integer (nullable = true)\n",
      " |-- OverTime: string (nullable = true)\n",
      " |-- Workex: integer (nullable = true)\n",
      " |-- YearsSinceLastPromotion: integer (nullable = true)\n",
      " |-- EmpSatisfaction: string (nullable = true)\n",
      " |-- TrainingTimesLastYear: integer (nullable = true)\n",
      " |-- WorkLifeBalance: string (nullable = true)\n",
      " |-- Performance_Rating: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hr_employee.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a4eed8",
   "metadata": {},
   "source": [
    "## 1. BigData file types\n",
    "#### a. Parquet File Format - records are stored columnunar in this format, compresses dataset of .csv of structured format into parquet format. Parquet format is good for query type of response\n",
    "###### [more about different file formats](https://www.clairvoyant.ai/blog/big-data-file-formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "055973d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_employee.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb461de",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_employee.write.parquet('file:///home/hadoop/Downloads/HR_Parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a1451",
   "metadata": {},
   "source": [
    "#### b. ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "244cb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_employee.write.orc('/HR_orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b24af33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+---------+------+---+-------------+-------------+--------------+-----------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "|EmployeeID|          Department|             JobRole|Attrition|Gender|Age|MaritalStatus|    Education|EducationField|   BusinessTravel|JobInvolvement|JobLevel|JobSatisfaction|Hourlyrate|Income|Salaryhike|OverTime|Workex|YearsSinceLastPromotion|EmpSatisfaction|TrainingTimesLastYear|WorkLifeBalance|Performance_Rating|\n",
      "+----------+--------------------+--------------------+---------+------+---+-------------+-------------+--------------+-----------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "|         1|               Sales|     Sales Executive|      Yes|Female| 41|       Single|      College| Life Sciences|    Travel_Rarely|          High|       2|      Very High|        94|  5993|        11|     Yes|     8|                      0|         Medium|                    0|            Bad|         Excellent|\n",
      "|         2|Research & Develo...|  Research Scientist|       No|  Male| 49|      Married|Below College| Life Sciences|Travel_Frequently|        Medium|       2|         Medium|        61|  5130|        23|      No|    10|                      1|           High|                    3|         Better|       Outstanding|\n",
      "|         3|Research & Develo...|Laboratory Techni...|      Yes|  Male| 37|       Single|      College|         Other|    Travel_Rarely|        Medium|       1|           High|        92|  2090|        15|     Yes|     7|                      0|      Very High|                    3|         Better|         Excellent|\n",
      "|         4|Research & Develo...|  Research Scientist|       No|Female| 33|      Married|       Master| Life Sciences|Travel_Frequently|          High|       1|           High|        56|  2909|        11|     Yes|     8|                      3|      Very High|                    3|         Better|         Excellent|\n",
      "|         5|Research & Develo...|Laboratory Techni...|       No|  Male| 27|      Married|Below College|       Medical|    Travel_Rarely|          High|       1|         Medium|        40|  3468|        12|      No|     6|                      2|            Low|                    3|         Better|         Excellent|\n",
      "|         6|Research & Develo...|Laboratory Techni...|       No|  Male| 32|       Single|      College| Life Sciences|Travel_Frequently|          High|       1|      Very High|        79|  3068|        13|      No|     8|                      3|      Very High|                    2|           Good|         Excellent|\n",
      "|         7|Research & Develo...|Laboratory Techni...|       No|Female| 59|      Married|     Bachelor|       Medical|    Travel_Rarely|     Very High|       1|            Low|        81|  2670|        20|     Yes|    12|                      0|           High|                    3|           Good|       Outstanding|\n",
      "|         8|Research & Develo...|Laboratory Techni...|       No|  Male| 30|     Divorced|Below College| Life Sciences|    Travel_Rarely|          High|       1|           High|        67|  2693|        22|      No|     1|                      0|      Very High|                    2|         Better|       Outstanding|\n",
      "|         9|Research & Develo...|Manufacturing Dir...|       No|  Male| 38|       Single|     Bachelor| Life Sciences|Travel_Frequently|        Medium|       3|           High|        44|  9526|        21|      No|    10|                      1|      Very High|                    2|         Better|       Outstanding|\n",
      "|        10|Research & Develo...|Healthcare Repres...|       No|  Male| 36|      Married|     Bachelor|       Medical|    Travel_Rarely|          High|       2|           High|        94|  5237|        13|      No|    17|                      7|           High|                    3|           Good|         Excellent|\n",
      "+----------+--------------------+--------------------+---------+------+---+-------------+-------------+--------------+-----------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.orc('/HR_orc/').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf14ea",
   "metadata": {},
   "source": [
    "## Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfbdb1b",
   "metadata": {},
   "source": [
    "#### 1. Optimizing spark jobs can significantly improve performance of Spark running queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50bb67d",
   "metadata": {},
   "source": [
    "## 2. Partitioning\n",
    "#### Partitioning divide data into smaller chunks, which can be processed parallely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cb30d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_employee.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "551de664",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_df = hr_employee.repartition(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a282a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_df.write.parquet('/HR_Partition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f7d15",
   "metadata": {},
   "source": [
    "## 3. Caching and Persistence\n",
    "#### Managing different levels of storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a927143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[EmployeeID: int, Department: string, JobRole: string, Attrition: string, Gender: string, Age: int, MaritalStatus: string, Education: string, EducationField: string, BusinessTravel: string, JobInvolvement: string, JobLevel: int, JobSatisfaction: string, Hourlyrate: int, Income: int, Salaryhike: int, OverTime: string, Workex: int, YearsSinceLastPromotion: int, EmpSatisfaction: string, TrainingTimesLastYear: int, WorkLifeBalance: string, Performance_Rating: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in-memory cache storage\n",
    "hr_employee.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83872c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistence of data storage with specific type of storage options like - memory only, memory_ser, or memeory_and_disk\n",
    "from pyspark import StorageLevel\n",
    "hr_employee1 = hr_employee.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4a8f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_employee2 = hr_employee.persist(StorageLevel.MEMORY_ONLY_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6051e1",
   "metadata": {},
   "source": [
    "## 4. Serialization\n",
    "#### Efficient Serialization reduces time to read/write data and transfer it over network.\n",
    "#### Kyro Serialization, Java Serialization  -- popular serialization method for betrer performance over defaulr jaa serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede50180",
   "metadata": {},
   "source": [
    "#### a. Java Serialization is the default serialization method. Easy to use, but drawback is that it will slow down the read-write process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df065076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f8bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Java Serialization').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "684c865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb780ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('PySpark serialization')\\\n",
    ".config('spark.serializer', 'org.apache.spark.serializer.JavaSerializer').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abfc07e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark serialization</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4bff518da0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71338dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### makes it faster and more compact than Java serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d278546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont run\n",
    "#spark = SparkSession.builder\\\n",
    "#.config('spark.Serializer', ;arg.apache.spark.serializer.KyroSerializer)\\\n",
    "#.config('spark.kyro.registrationRequired', 'True')\\\n",
    "#.config('spark.kyro.classesToRegister', 'org.apache.spark.example.person')\\\n",
    "#\n",
    "##.appName('Kyro Serialization').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2141cd",
   "metadata": {},
   "source": [
    "## 5. Broadcast Join\n",
    "#### increase size of smaller one then join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b7c728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = spark.read.csv('file:///home/hadoop/Downloads/airports.csv')\n",
    "df = spark.read.csv('file:///home/hadoop/Downloads/raw_flight_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4221a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "broadcast_df = broadcast(small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2ded429",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_df = broadcast_df.cache()\n",
    "df = df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ee0c568",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'OriginAirportID'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-be8ac89aadb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# broadcast join\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mairport_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOriginAirportID\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mbroadcast_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mairport_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1305\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1306\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'OriginAirportID'"
     ]
    }
   ],
   "source": [
    "# broadcast join\n",
    "airport_df = df.join(broadcast_df, df.OriginAirportID==broadcast_df.airport_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ba45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25318031",
   "metadata": {},
   "source": [
    "## 6. Level of Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a9c78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust level of parallelism base don cluster size\n",
    "spark.conf.set('spark.default.parallelism', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161258e8",
   "metadata": {},
   "source": [
    "## 7. Avoid GroupByKey()\n",
    "#### use ReduceByKey() or aggregateByKey() instead of GroupByKey() to reduce number of shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da181fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dosa', 2), ('idli', 7), ('vada', 8), ('rice', 1), ('coffee', 5)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([('dosa',2), ('idli',3), ('vada',3), ('rice',1), ('coffee',5), ('idli',4), ('vada',5)])\n",
    "\n",
    "# option, this method is to be avoided\n",
    "#rdd.groupByKey().mapValues(sum).collect()\n",
    "\n",
    "rdd.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a73ad86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "| order|total_value|\n",
      "+------+-----------+\n",
      "|  vada|          8|\n",
      "|  dosa|          2|\n",
      "|  rice|          1|\n",
      "|  idli|          7|\n",
      "|coffee|          5|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df = spark.createDataFrame([('dosa',2), ('idli',3), ('vada',3), ('rice',1), ('coffee',5), ('idli',4), \n",
    "                            ('vada',5)], schema=['order', 'value'])\n",
    "df.groupBy('order').agg(sum('value').alias('total_value')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00caf38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dosa', 2), ('idli', 7), ('vada', 8), ('rice', 1), ('coffee', 5)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0f4f8",
   "metadata": {},
   "source": [
    "## 8. Reduce Shuffle\n",
    "#### Reduce number of shuffles by optimizing transformations\n",
    "#### Use reduceByKey() over groupByKey()\n",
    "#### use map() and reduce() over groupBy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8efd63",
   "metadata": {},
   "source": [
    "## 9. Repartition and Coalesce\n",
    "    * Use accumulators for optimizing aggregate information like count(), sum() across all executors parallely executing tasks in multiple worker nodes.\n",
    "    * accumulator in Spark are variables, that can be added through cumulative operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37894598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare and initialize Accumulator\n",
    "acc = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9cc3ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.accumulators.Accumulator"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30801b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff7336c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python UDF\n",
    "def add(x):\n",
    "    acc.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c1bdd26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.foreach(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "227841fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "print(acc.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aaa31d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = spark.sparkContext.accumulator(0)\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "def counter(x):\n",
    "    # global acc  # this was used earlier when this block didnt reinitialize acc and used previous one from above\n",
    "    acc.add(1)\n",
    "    # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43349549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "rdd.foreach(counter)\n",
    "print(acc.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69ce2b",
   "metadata": {},
   "source": [
    "## 10. Bucketing\n",
    "    * use bucketing to create large datasets for efficient queries as well as joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d8cb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
